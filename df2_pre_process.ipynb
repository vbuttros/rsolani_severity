{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aIblG-P3HA6s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_auc_score,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from skbio.diversity import alpha_diversity, beta_diversity\n",
    "from skbio.stats.ordination import pcoa\n",
    "from skbio.diversity.alpha import shannon\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LaLUoGWHHcxO"
   },
   "outputs": [],
   "source": [
    "# Dataset 2\n",
    "data = pd.read_csv('/Users/vbuttros/Library/CloudStorage/OneDrive-Personal/University/Microbiology - DSc/Colaborations/Buttros, VH/R_solanii_ANN/Treated_data/dataset2_script_files/data.csv')\n",
    "\n",
    "otu_table_16S = pd.read_csv(\n",
    "    '/Users/vbuttros/Library/CloudStorage/OneDrive-Personal/University/Microbiology - DSc/Colaborations/Buttros, VH/R_solanii_ANN/Treated_data/dataset2_script_files/OTU_table_16S.csv',\n",
    "    header=0,  # Use the first row as headers\n",
    "    index_col=0  # Set the first column as the index\n",
    ")\n",
    "\n",
    "otu_table_ITS = pd.read_csv(\n",
    "    '/Users/vbuttros/Library/CloudStorage/OneDrive-Personal/University/Microbiology - DSc/Colaborations/Buttros, VH/R_solanii_ANN/Treated_data/dataset2_script_files/OTU_table_ITS.csv',\n",
    "    header=0,  # Use the first row as headers\n",
    "    index_col=0  # Set the first column as the index\n",
    ")\n",
    "\n",
    "equivalence_table = pd.read_csv('/Users/vbuttros/Library/CloudStorage/OneDrive-Personal/University/Microbiology - DSc/Colaborations/Buttros, VH/R_solanii_ANN/Treated_data/dataset2_script_files/eq_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Y_rs.dis with otu_table_16S\n",
    "sev_16S = otu_table_16S.merge(data[['Sample_ID', 'Y_rs.dis']], on='Sample_ID', how='inner')\n",
    "sev_16S.set_index(sev_16S.columns[0], inplace=True)  # Set column 0 as the index\n",
    "\n",
    "# Merge Y_rs.dis with otu_table_ITS\n",
    "sev_ITS = otu_table_ITS.merge(data[['Sample_ID', 'Y_rs.dis']], on='Sample_ID', how='inner')\n",
    "sev_ITS.set_index(sev_ITS.columns[0], inplace=True)  # Set column 0 as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_regression(data, otu_type, n_estimators=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform Random Forest regression to calculate feature importance, prioritizing OTUs that reduce the response.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing predictors and response variable.\n",
    "        otu_type (str): Type of OTU ('fungal' or 'bacterial') to reflect in output name.\n",
    "        n_estimators (int): Number of trees in the random forest (default: 100).\n",
    "        random_state (int): Random seed for reproducibility (default: 42).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing OTU and Feature Importance.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    # Normalize all columns to a 0-1 range\n",
    "    scaler = MinMaxScaler()\n",
    "    data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "\n",
    "    # Extract response variable (Y_rs.dis) and predictors (all other columns)\n",
    "    response = data_normalized['Y_rs.dis']\n",
    "    predictors = data_normalized.drop(columns=['Y_rs.dis'])\n",
    "\n",
    "    # Apply inverse weighting to the response to prioritize smaller values\n",
    "    response_weighted = 1 / (response + 1e-6)  # Adding a small constant to avoid division by zero\n",
    "\n",
    "    # Fit Random Forest regressor using weighted response\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state, n_jobs=-1)\n",
    "    rf_model.fit(predictors, response_weighted)\n",
    "\n",
    "    # Use built-in feature importance\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame for the results\n",
    "    importance_results = pd.DataFrame({\n",
    "        'OTU': predictors.columns,\n",
    "        'Feature_Importance': feature_importance\n",
    "    })\n",
    "\n",
    "    # Sort by importance\n",
    "    importance_results = importance_results.sort_values(by='Feature_Importance', ascending=False)\n",
    "\n",
    "    # Name the output DataFrame based on OTU type\n",
    "    importance_results.name = f\"{otu_type}_rf_reg\"\n",
    "\n",
    "    return importance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_sev_ITS = perform_regression(sev_ITS, 'fungal')\n",
    "reg_sev_16S = perform_regression(sev_16S, 'bacterial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_proc(feature_importance_results):\n",
    "    \"\"\"\n",
    "    Process feature importance results by normalizing to a 0-1 scale.\n",
    "\n",
    "    Parameters:\n",
    "        feature_importance_results (pd.DataFrame): DataFrame containing 'OTU' and 'Feature_Importance'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with a new column 'Normalized_Importance'.\n",
    "    \"\"\"\n",
    "    # Normalize the feature importance to a 0-1 range\n",
    "    min_val = feature_importance_results['Feature_Importance'].min()\n",
    "    max_val = feature_importance_results['Feature_Importance'].max()\n",
    "    \n",
    "    feature_importance_results['Normalized_Importance'] = (\n",
    "        (feature_importance_results['Feature_Importance'] - min_val) / \n",
    "        (max_val - min_val)\n",
    "    )\n",
    "\n",
    "    return feature_importance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_sev_16S = reg_proc(reg_sev_16S)\n",
    "reg_sev_ITS = reg_proc(reg_sev_ITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bcon_index(feature_importance_results, abundance_table, otu_type, operation='*', amplify_factor=0.1, final_amplification=0.1):\n",
    "    \"\"\"\n",
    "    Calculate the Biocontrol Index (BCI) with enhanced emphasis on its predictive importance for disease severity.\n",
    "\n",
    "    Parameters:\n",
    "        feature_importance_results (pd.DataFrame): DataFrame containing OTUs and their 'Normalized_Importance'.\n",
    "        abundance_table (pd.DataFrame): Abundance table with OTUs as columns and Sample_ID as index.\n",
    "        otu_type (str): Type of OTU ('fungal' or 'bacterial') to name the output column.\n",
    "        operation (str): Operation to apply for BCI calculation ('*', '2', '3', etc.).\n",
    "        amplify_factor (int): Exponent to amplify the effect of Normalized Importance.\n",
    "        final_amplification (float): Factor to amplify final BCI values for better interpretability.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with Sample_ID as rows and BCI values as a column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure 'Normalized_Importance' column exists\n",
    "    if 'Normalized_Importance' not in feature_importance_results.columns:\n",
    "        raise ValueError(\"The feature importance results must include a 'Normalized_Importance' column.\")\n",
    "\n",
    "    # Filter OTUs with non-zero Normalized Importance\n",
    "    significant_otus = feature_importance_results[\n",
    "        feature_importance_results['Normalized_Importance'] > 0\n",
    "    ]\n",
    "\n",
    "    # Map Normalized Importance to the abundance table columns\n",
    "    importance_dict = significant_otus.set_index('OTU')['Normalized_Importance'].to_dict()\n",
    "\n",
    "    # Ensure only relevant OTUs are used\n",
    "    filtered_abundance_table = abundance_table[\n",
    "        abundance_table.columns.intersection(importance_dict.keys())\n",
    "    ]\n",
    "\n",
    "    # Prepare values for calculations\n",
    "    importance_values = filtered_abundance_table.columns.map(importance_dict)\n",
    "\n",
    "    # Perform the specified operation\n",
    "    if operation == '*':  # Multiplication\n",
    "        bci_values = filtered_abundance_table.multiply(importance_values**amplify_factor, axis=1).sum(axis=1)\n",
    "    elif operation == '2':  # Weighted sum of squares\n",
    "        bci_values = (\n",
    "            (filtered_abundance_table**2).multiply(importance_values**amplify_factor, axis=1).sum(axis=1)\n",
    "        )\n",
    "    elif operation == '3':  # Sum of weighted abundances\n",
    "        bci_values = filtered_abundance_table.multiply(importance_values**amplify_factor, axis=1).sum(axis=1)\n",
    "    elif operation == '4':  # Normalized importance scaled by abundance\n",
    "        bci_values = filtered_abundance_table.div(importance_values**amplify_factor + 1e-8, axis=1).sum(axis=1)\n",
    "    elif operation == '5':  # Sum of log-weighted contributions\n",
    "        bci_values = np.log(filtered_abundance_table + 1).multiply(importance_values**amplify_factor, axis=1).sum(axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operation. Choose from '*', '2', '3', '4', '5'.\")\n",
    "\n",
    "    # Amplify the BCI values for better interpretability\n",
    "    bci_values *= final_amplification\n",
    "\n",
    "    # Create a DataFrame with the BCI values\n",
    "    bci_df = pd.DataFrame(bci_values, columns=[f\"{otu_type}_BCI\"])\n",
    "    bci_df.index.name = 'Sample_ID'\n",
    "\n",
    "    return bci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacterial_bci = bcon_index(reg_sev_16S, otu_table_16S, 'bacterial', operation='4')\n",
    "fungal_bci = bcon_index(reg_sev_ITS, otu_table_ITS, 'fungal', operation='4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bci_to_data(data, bacterial_bci, fungal_bci):\n",
    "    \"\"\"\n",
    "    Add bacterial_BCI and fungal_BCI columns to the data, prefixed with 'P_'.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Original data with 'Sample_ID' as a column.\n",
    "        bacterial_bci (pd.DataFrame): DataFrame containing bacterial_BCI values with Sample_ID as the index.\n",
    "        fungal_bci (pd.DataFrame): DataFrame containing fungal_BCI values with Sample_ID as the index.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated data with P_bacterial_BCI and P_fungal_BCI columns.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Merge bacterial BCI into the data\n",
    "    data = data.merge(bacterial_bci, left_on='Sample_ID', right_index=True, how='left')\n",
    "    \n",
    "    # Merge fungal BCI into the data\n",
    "    data = data.merge(fungal_bci, left_on='Sample_ID', right_index=True, how='left')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = add_bci_to_data(data, bacterial_bci, fungal_bci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_bci_scores(data, equivalence_table):\n",
    "    \"\"\"\n",
    "    Fill missing BCI scores using equivalent samples from the equivalence table.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Original data containing 'Sample_ID', 'Fungal_BCI', and 'Bacterial_BCI'.\n",
    "        equivalence_table (pd.DataFrame): DataFrame with columns 'fungal_Sample_ID' and 'bacterial_Sample_ID'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated data with BCI columns filled based on equivalence.\n",
    "    \"\"\"\n",
    "    # Create mappings for equivalence\n",
    "    fungal_to_bacterial = equivalence_table.set_index('fungal_Sample_ID')['bacterial_Sample_ID'].to_dict()\n",
    "    bacterial_to_fungal = equivalence_table.set_index('bacterial_Sample_ID')['fungal_Sample_ID'].to_dict()\n",
    "\n",
    "    # Fill missing Bacterial_BCI using bacterial equivalents\n",
    "    for idx, row in data.iterrows():\n",
    "        if pd.isna(row['bacterial_BCI']):\n",
    "            sample_id = row['Sample_ID']\n",
    "            if sample_id in fungal_to_bacterial:  # Check for fungal sample's equivalent bacterial sample\n",
    "                equivalent_id = fungal_to_bacterial[sample_id]\n",
    "                equivalent_row = data.loc[data['Sample_ID'] == equivalent_id]\n",
    "                if not equivalent_row.empty:  # Ensure the equivalent bacterial row exists\n",
    "                    data.at[idx, 'bacterial_BCI'] = equivalent_row['bacterial_BCI'].values[0]\n",
    "\n",
    "    # Fill missing Fungal_BCI using fungal equivalents\n",
    "    for idx, row in data.iterrows():\n",
    "        if pd.isna(row['fungal_BCI']):\n",
    "            sample_id = row['Sample_ID']\n",
    "            if sample_id in bacterial_to_fungal:  # Check for bacterial sample's equivalent fungal sample\n",
    "                equivalent_id = bacterial_to_fungal[sample_id]\n",
    "                equivalent_row = data.loc[data['Sample_ID'] == equivalent_id]\n",
    "                if not equivalent_row.empty:  # Ensure the equivalent fungal row exists\n",
    "                    data.at[idx, 'fungal_BCI'] = equivalent_row['fungal_BCI'].values[0]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = duplicate_bci_scores(merged_data, equivalence_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eco_metrics(abundance_table, otu_type, metric=\"braycurtis\"):\n",
    "    \"\"\"\n",
    "    Calculate ecological metrics (alpha diversity, evenness, and beta diversity) for microbial communities.\n",
    "\n",
    "    Parameters:\n",
    "        abundance_table (pd.DataFrame): Abundance table with OTUs as columns and Sample_ID as index.\n",
    "        otu_type (str): Type of OTU ('fungal' or 'bacterial') to name the output columns.\n",
    "        metric (str): Distance metric for beta diversity (default: 'braycurtis').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with alpha diversity, beta diversity, and evenness as columns.\n",
    "    \"\"\"\n",
    "    # Ensure no empty rows\n",
    "    abundance_table = abundance_table.loc[abundance_table.sum(axis=1) > 0]\n",
    "    \n",
    "    # Alpha diversity (Shannon Index)\n",
    "    alpha_div = abundance_table.apply(lambda row: shannon(row.values), axis=1)\n",
    "    \n",
    "    # Evenness calculation (Pielou's evenness)\n",
    "    total_abundance = abundance_table.sum(axis=1)\n",
    "    evenness = alpha_div / np.log(total_abundance.replace(0, 1))\n",
    "    \n",
    "    # Beta diversity (distance matrix)\n",
    "    beta_div = beta_diversity(metric, abundance_table, ids=abundance_table.index)\n",
    "    beta_div_mean = beta_div.to_data_frame().mean(axis=1)  # Calculate mean beta diversity for each sample\n",
    "\n",
    "    # Construct the output DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        f\"{otu_type}_alpha_diversity\": alpha_div,\n",
    "        f\"{otu_type}_evenness\": evenness,\n",
    "        f\"{otu_type}_beta_diversity\": beta_div_mean\n",
    "    }, index=abundance_table.index)\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacterial_metrics = eco_metrics(otu_table_16S, otu_type='bacterial')\n",
    "fungal_metrics = eco_metrics(otu_table_ITS, otu_type='fungal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metrics_to_data(data, bacterial_metrics, fungal_metrics):\n",
    "    \"\"\"\n",
    "    Add bacterial and fungal ecological metrics to the data, with columns prefixed by 'P_'.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Original data with 'Sample_ID' as a column.\n",
    "        bacterial_metrics (pd.DataFrame): DataFrame containing bacterial metrics with Sample_ID as the index.\n",
    "        fungal_metrics (pd.DataFrame): DataFrame containing fungal metrics with Sample_ID as the index.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated data with prefixed ecological metrics columns.\n",
    "    \"\"\"\n",
    "    # Ensure prefix for bacterial metrics\n",
    "    bacterial_metrics = bacterial_metrics.add_prefix(\"P_\")\n",
    "    \n",
    "    # Ensure prefix for fungal metrics\n",
    "    fungal_metrics = fungal_metrics.add_prefix(\"P_\")\n",
    "    \n",
    "    # Merge bacterial metrics into the data\n",
    "    data = data.merge(bacterial_metrics, left_on='Sample_ID', right_index=True, how='left')\n",
    "    \n",
    "    # Merge fungal metrics into the data\n",
    "    data = data.merge(fungal_metrics, left_on='Sample_ID', right_index=True, how='left')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = add_metrics_to_data(merged_data, bacterial_metrics, fungal_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_metrics(data, bacterial_metrics, fungal_metrics, equivalence_table):\n",
    "    \"\"\"\n",
    "    Duplicate equivalent metrics to fill NaN values based on the equivalence table.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Original data containing 'Sample_ID' as a column.\n",
    "        bacterial_metrics (pd.DataFrame): DataFrame with bacterial metrics indexed by Sample_ID.\n",
    "        fungal_metrics (pd.DataFrame): DataFrame with fungal metrics indexed by Sample_ID.\n",
    "        equivalence_table (pd.DataFrame): DataFrame with columns 'fungal_Sample_ID' and 'bacterial_Sample_ID'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated data with bacterial and fungal metrics filled based on equivalence.\n",
    "    \"\"\"\n",
    "    # Merge bacterial and fungal metrics into the main data\n",
    "    data = data.merge(bacterial_metrics, left_on='Sample_ID', right_index=True, how='left')\n",
    "    data = data.merge(fungal_metrics, left_on='Sample_ID', right_index=True, how='left')\n",
    "\n",
    "    # Create mappings from the equivalence table\n",
    "    fungal_to_bacterial = equivalence_table.set_index('fungal_Sample_ID')['bacterial_Sample_ID'].to_dict()\n",
    "    bacterial_to_fungal = equivalence_table.set_index('bacterial_Sample_ID')['fungal_Sample_ID'].to_dict()\n",
    "\n",
    "    # Fill missing fungal metrics using only fungal metrics from equivalent fungal samples\n",
    "    for idx, row in data.iterrows():\n",
    "        if pd.isna(row['fungal_alpha_diversity']):  # Check if fungal metric is missing\n",
    "            sample_id = row['Sample_ID']\n",
    "            if sample_id in bacterial_to_fungal:  # Find bacterial equivalent\n",
    "                equivalent_id = bacterial_to_fungal[sample_id]\n",
    "                equivalent_row = data.loc[data['Sample_ID'] == equivalent_id]\n",
    "                if not equivalent_row.empty:  # Ensure the equivalent row exists\n",
    "                    # Copy only fungal metrics\n",
    "                    for col in data.filter(like=\"fungal_\").columns:\n",
    "                        data.at[idx, col] = equivalent_row[col].values[0]\n",
    "\n",
    "    # Fill missing bacterial metrics using only bacterial metrics from equivalent bacterial samples\n",
    "    for idx, row in data.iterrows():\n",
    "        if pd.isna(row['bacterial_alpha_diversity']):  # Check if bacterial metric is missing\n",
    "            sample_id = row['Sample_ID']\n",
    "            if sample_id in fungal_to_bacterial:  # Find fungal equivalent\n",
    "                equivalent_id = fungal_to_bacterial[sample_id]\n",
    "                equivalent_row = data.loc[data['Sample_ID'] == equivalent_id]\n",
    "                if not equivalent_row.empty:  # Ensure the equivalent row exists\n",
    "                    # Copy only bacterial metrics\n",
    "                    for col in data.filter(like=\"bacterial_\").columns:\n",
    "                        data.at[idx, col] = equivalent_row[col].values[0]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = duplicate_metrics(merged_data, bacterial_metrics, fungal_metrics, equivalence_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_p_prefix(data):\n",
    "    \"\"\"\n",
    "    Add the prefix 'P_' to column names in the DataFrame that start with 'fungal' or 'bacterial'\n",
    "    and do not already start with 'P_'.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with updated column names.\n",
    "    \"\"\"\n",
    "    # Update column names\n",
    "    data = data.rename(\n",
    "        columns=lambda col: f\"P_{col}\" if col.startswith(('fungal', 'bacterial')) and not col.startswith('P_') else col\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = add_p_prefix(merged_data)\n",
    "merged_data.set_index(merged_data.columns[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_samples(data, predictor_columns):\n",
    "    \"\"\"\n",
    "    Removes one occurrence of rows with duplicate values across all predictor columns.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The input DataFrame containing predictors and other columns.\n",
    "    - predictor_columns (list): A list of predictor column names to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with duplicates removed but one instance retained.\n",
    "    \"\"\"\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    filtered_data = data[~data.duplicated(subset=predictor_columns, keep='first')]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove exact duplicate columns\n",
    "merged_data = merged_data.loc[:, ~merged_data.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new columns by calculating the square root of the sum of squares of bacterial and fungal columns\n",
    "merged_data.loc[:, 'P_BCI'] = merged_data['P_bacterial_BCI']**2 + merged_data['P_fungal_BCI']**2\n",
    "merged_data.loc[:, 'P_alpha_diversity'] = (merged_data['P_bacterial_alpha_diversity']**2 + merged_data['P_fungal_alpha_diversity']**2)**0.5\n",
    "merged_data.loc[:, 'P_evenness'] = (merged_data['P_bacterial_evenness']**2 + merged_data['P_fungal_evenness']**2)**0.5\n",
    "merged_data.loc[:, 'P_beta_diversity'] = (merged_data['P_bacterial_beta_diversity']**2 + merged_data['P_fungal_beta_diversity']**2)**0.5\n",
    "\n",
    "# Drop the original bacterial and fungal columns\n",
    "merged_data = merged_data.drop(columns=[\n",
    "    'P_bacterial_BCI', 'P_fungal_BCI',\n",
    "    'P_bacterial_alpha_diversity', 'P_fungal_alpha_diversity',\n",
    "    'P_bacterial_evenness', 'P_fungal_evenness',\n",
    "    'P_bacterial_beta_diversity', 'P_fungal_beta_diversity'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [col for col in merged_data.columns if col.startswith('K_')]\n",
    "predictors = [col for col in merged_data.columns if col.startswith('P_')]\n",
    "target = [col for col in merged_data.columns if col.startswith('Y_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = remove_duplicate_samples(merged_data, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PwSKSl68Hujw"
   },
   "outputs": [],
   "source": [
    "classifiers = [col for col in merged_data.columns if col.startswith('K_')]\n",
    "predictors = [col for col in merged_data.columns if col.startswith('P_')]\n",
    "target = [col for col in merged_data.columns if col.startswith('Y_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wGBo3k21KY0o"
   },
   "outputs": [],
   "source": [
    "# Impute missing values for predictors\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "merged_data[predictors] = imputer.fit_transform(merged_data[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale predictors\n",
    "scaler = StandardScaler()\n",
    "merged_data[predictors] = scaler.fit_transform(merged_data[predictors])\n",
    "#merged_data[target] = scaler.fit_transform(merged_data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The midpoint of Y_rs.dis is: 57.142857145\n"
     ]
    }
   ],
   "source": [
    "# response midpoint\n",
    "min_value = merged_data['Y_rs.dis'].min()\n",
    "max_value = merged_data['Y_rs.dis'].max()\n",
    "\n",
    "midpoint = (min_value + max_value) / 2\n",
    "\n",
    "print(f\"The midpoint of Y_rs.dis is: {midpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dataset  Number of Variables  Number of Samples  Number of OTUs\n",
      "0  Main Data                   22                160             NaN\n",
      "1   16S OTUs                14652                 80         14652.0\n",
      "2   ITS OTUs                 1402                 80          1402.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate descriptive metrics\n",
    "def calculate_descriptive_metrics(data, otu_table_16S, otu_table_ITS):\n",
    "    descriptive_metrics = {\n",
    "        \"Dataset\": [\"Main Data\", \"16S OTUs\", \"ITS OTUs\"],\n",
    "        \"Number of Variables\": [data.shape[1], otu_table_16S.shape[1], otu_table_ITS.shape[1]],\n",
    "        \"Number of Samples\": [data.shape[0], otu_table_16S.shape[0], otu_table_ITS.shape[0]],\n",
    "        \"Number of OTUs\": [None, otu_table_16S.shape[1], otu_table_ITS.shape[1]],\n",
    "    }\n",
    "    return pd.DataFrame(descriptive_metrics)\n",
    "\n",
    "# Generate the descriptive metrics table\n",
    "descriptive_metrics_table = calculate_descriptive_metrics(data, otu_table_16S, otu_table_ITS)\n",
    "\n",
    "# Display the descriptive metrics\n",
    "print(descriptive_metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "output_path = '/Users/vbuttros/Library/CloudStorage/OneDrive-Personal/University/Microbiology - DSc/Colaborations/Buttros, VH/R_solanii_ANN/Treated_data/dataset2_script_files/merged_data.csv'\n",
    "merged_data.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
